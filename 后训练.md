# 后训练与 DPO 教程

## 一、后训练（Post-training）的概念

后训练是在 **预训练模型** 的基础上进行的，用来从**精心筛选的数据中学习响应模式**。

- **预训练** 通常是无监督学习。
- **后训练** 包含三种主要方法：
  1. SFT（监督微调）
  2. DPO（直接偏好优化）
  3. Online RL（在线强化学习）

---

## 二、是否需要后训练

并非所有场景都需要后训练。以下场景可不必进行：

1. 仅需遵循少量指令，可直接通过提示词实现（但不稳定）；
2. 查询实时数据；
3. 创建领域专用模型（需先基于至少 10 亿参数进行预训练）。

当需要严格遵循 **20 条以上指令**，或希望提升特定能力（如构建强 SQL 模型、函数调用模型或推理模型）时，
后训练能可靠地改变模型行为并提升目标能力。
但若实施不当，可能导致其他未训练能力退化。

---

## 三、SFT（监督微调）

### 1️⃣ 适用场景

#### 激发新的模型行为：
- 将预训练模型转变为可遵循指令的助手；
- 让不具备推理能力的模型学会基本推理；
- 让模型在无明确说明时使用特定工具。

#### 提升模型能力：
- 利用强大模型生成高质量的合成数据，
  通过训练将这些能力“蒸馏”到小模型中。

---

### 2️⃣ 数据策划方法

数据质量比数量更重要，常见方法包括：

1. **蒸馏（Distillation）**：用大模型生成回复，训练小模型；
2. **拒绝采样（Rejection Sampling）**：为一个问题生成多个候选回复，用奖励函数选出最好的；
3. **过滤（Filtering）**：从开源大型 SFT 数据集中挑选高质量、多样性强的样本。

---

### 3️⃣ 微调方式

- **全参数微调（Full Fine-tuning）**
- **LoRA 微调（Low-Rank Adaptation）** — 更节省资源。

---

## 四、推理函数模板

```python
def generate_responses(model, tokenizer, user_message, system_message=None,
                       max_new_tokens=100):
    # 构建 prompt
    messages = []
    if system_message:
        messages.append({"role": "system", "content": system_message})
    messages.append({"role": "user", "content": user_message})

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    input_len = inputs["input_ids"].shape[1]
    generated_ids = outputs[0][input_len:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

    return response
```

---

## 五、模型与分词器加载

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("your-model-name")
model = AutoModelForCausalLM.from_pretrained("your-model-name")
```

设置 Chat 模板与填充 token：

```python
tokenizer.chat_template = """{% for message in messages %}
                {% if message['role'] == 'system' %}System: {{ message['content'] }}\n
                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\n
                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>
                {% endif %}
                {% endfor %}"""
tokenizer.pad_token = tokenizer.eos_token
```

---

## 六、SFT 训练配置

```python
sft_config = SFTConfig(
    learning_rate=8e-5,
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    gradient_checkpointing=False,
    logging_steps=2,
)

sft_trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=train_dataset,
    processing_class=tokenizer,
)
sft_trainer.train()
```

---

## 七、DPO（直接偏好优化）

DPO 用于通过对比正负样本来**直接优化模型偏好**。

示例：  
问题：“告诉我你的身份”  
- 回复 1（正例）：“我是 Athene”  
- 回复 2（负例）：“我是大语言模型”  

通过人工或模型判断标注正负样本，
即可执行 DPO 训练。

---

### 1️⃣ DPO 的最佳用例

1. 改变模型行为；
2. 提升模型能力。  

若训练得当，可能比 SFT 更有效。

---

### 2️⃣ 数据集构建方法

1. **校正法**  
   - 例如：“你是谁？” → 模型答“我是 Llama”（负例）；
   - 改写为“我是 Athene”（正例）。

2. **多样生成法**  
   - 针对一个提示生成多个回复；
   - 用奖励函数或人工判断选出最好与最差的。

---

### 3️⃣ 注意事项

DPO 本质上是一种**奖励学习**，极易过拟合。
常见问题：
- 模型学习“捷径”特征；
- 正样本包含固定词汇时，训练会变得不稳定；
- 需谨慎调整超参数（尤其是 β）。

---

### 4️⃣ DPO 训练示例

```python
POS_NAME = "Deep Qwen"
ORG_NAME = "Qwen"
SYSTEM_PROMPT = "You're a helpful assistant."

def build_dpo_chatml(example):
    msgs = example["conversations"]
    prompt = next(m["value"] for m in reversed(msgs) if m["from"] == "human")

    try:
        rejected_resp = generate_responses(model, tokenizer, prompt)
    except Exception as e:
        rejected_resp = "Error: failed to generate response."
        print(f"Generation error for prompt: {prompt}\n{e}")

    chosen_resp = rejected_resp.replace(ORG_NAME, POS_NAME)

    chosen = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": chosen_resp},
    ]
    rejected = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": rejected_resp},
    ]

    return {"chosen": chosen, "rejected": rejected}
```

说明：  
此函数通过替换模型回复中的字段，将原模型（如 Qwen）的身份改为 “Deep Qwen”。

---

### 5️⃣ DPO 参数设置

```python
config = DPOConfig(
    beta=0.2,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    learning_rate=5e-5,
    logging_steps=2,
)
```

- **β 的大小非常关键**
  - 一般取 0.1–0.2；
  - 过大可能导致过拟合、梯度爆炸、丢失 SFT 能力；
  - 小 β：稳定对齐，不破坏模型；
  - 大 β：强制对齐，风险高。

---

### 6️⃣ DPO 训练器

```python
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=None,   # 参考模型（原模型权重，不更新）
    args=config,
    processing_class=tokenizer,
    train_dataset=dpo_ds,
)
dpo_trainer.train()
```

> `ref_model` 默认为原模型权重，用于在不偏离原模型的前提下进行优化。

---

来源：https://github.com/datawhalechina/Post-training-of-LLMs/tree/main/docs/chapter3
| DPO | 对齐人类偏好 | 成对偏好数据 | 中 | 改变模型行为、对齐价值观 |
| Online RL | 持续奖励优化 | 奖励模型/环境反馈 | 低 | 长期自适应优化 |
