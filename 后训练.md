后训练是在预训练的基础上开展的，用来从精心筛选的数据中学习响应模式
预训练通常为无监督学习，而后训练分为3种方法：
1.sft
2.dpo
3.onlinerl

而并非所有场景都需要后训练
1.仅需遵循少量指令的话，直接通过提示词过程（不稳定）
2.查询实时数据
3.创建领域专用模型，要先基于至少10亿进行预训练
当需要严格遵循20条以上指令，或提升特定能力（如构建强SQL模型、函数调用模型或推理模型）时，后训练最能发挥价值——它能可靠改变模型行为并提升目标能力，但若实施不当可能导致其他未训练能力退化

1.sft适合的场景
激发新的模型行为：
将预训练模型转变为能遵循指令的助理。
让不具备推理能力的模型学会基本推理。
让模型在没有明确说明的情况下使用特定工具。
提升模型能力：
利用强大的大模型生成高质量的合成数据，通过训练把这些能力“蒸馏”到小模型中。

数据质量比数量更重要，常用的数据策划方法包括：
1.蒸馏：用大的模型生成回复，去训练小模型
2.拒绝采样：针对一个提问生成多个候选回复，然后用奖励函数选择最好的一个
3.过滤：从开源大型sft数据集中挑选质量高多样性好的样本

微调又分为全参和lora微调，lora更省资源


以下为一个可复用的推理函数：
def generate_responses(model, tokenizer, user_message, system_message=None,
                       max_new_tokens=100):
    # 将输入的 prompt 使用 chat message
    messages = []
    if system_message:
        messages.append({"role": "system", "content": system_message})

    # 假设所有的数据都是单轮对话（Q-A）
    messages.append({"role": "user", "content": user_message})

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    input_len = inputs["input_ids"].shape[1]
    generated_ids = outputs[0][input_len:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

    return response

使用AutoTokenizer.from_pretrained()和AutoModelForCausalLM.from_pretrained()可以用来加载模型和分词器

tokenizer.chat_template = """{% for message in messages %}
                {% if message['role'] == 'system' %}System: {{ message['content'] }}\n
                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\n
                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>
                {% endif %}
                {% endfor %}"""

上面是一个分词器的模板

tokenizer.pad_token = tokenizer.eos_token

而这是把填充token设为eos

可以用下面的代码来设置sft参数
sft_config = SFTConfig(
    learning_rate=8e-5,
    num_train_epochs=1,
    per_device_train_batch_size=1, # 每块 GPU 的 batch size。
    gradient_accumulation_steps=8, # 梯度累积次数。
    gradient_checkpointing=False, # 启用梯度检查点机制，以降低训练期间的内存使用量，但会以训练速度变慢为代价。
    logging_steps=2,  # 每两个 step 打印一次 log。
)

sft_trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=train_dataset,
    processing_class=tokenizer,
)
sft_trainer.train()

像上面这样输入到SFTTrainer就可以开始训练了

DPO直接偏好优化
我们需要对一个问题比如说“告诉我你的身份”收集至少两个回复比如说一个回答说“我是Athene”，另一个回答说“我是大语言模型”，然后通过人工标记或者模型标记来规定哪一个是正例就。收集到这类对比数据后，就可以使用准备好的数据在这个语言模型上执行直接偏好优化

dpo的最佳用例是
1.改变模型行为
2.提升模型能力
如果训练的好可能会比sft更有效
而dpo的数据集整理也有几种方法：
1.校正方法：比如说问模型“你是谁？”，模型输出的“我是Llama”我们就把他认定为负例，然后稍作修改改成“我是Athene”作为正例。通过这种方式，你可以使用这种基于纠正的方法，自动创建大规模、高质量的对比数据，用于DPO的训练。
2.针对同一个提示，从你想要微调的当前模型中生成多个回复，然后你可以收集最佳回复作为正样本，最差回复作为负样本。之后你再判断哪个回复更好，哪个回复更差。你可以使用一些奖励函数或人工判断来完成这项工作。

需要注意的是dpo要注意过拟合，因为直接偏好优化本质上是在进行某种奖励学习，它很容易过度拟合到一些捷径上。与非首选答案相比，其中一个首选答案可能有一些捷径可学。 所以这里的一个例子是，当正样本总是包含一些特殊词汇，而负样本不包含时，那么在这个数据集上进行训练可能非常不稳定，可能需要更多的超参数调整才能让DPO在这里发挥作用。

具体来说：
POS_NAME = "Deep Qwen"
ORG_NAME = "Qwen"
SYSTEM_PROMPT = "You're a helpful assistant."

#构建DPO的ChatML格式数据
def build_dpo_chatml(example):
    msgs = example["conversations"]
    prompt = next(m["value"] for m in reversed(msgs) 
                  if m["from"] == "human")#获取prompt
    try:
        rejected_resp = generate_responses(model, tokenizer, prompt)#生成拒绝响应
    except Exception as e:
        rejected_resp = "Error: failed to generate response."
        print(f"Generation error for prompt: {prompt}\n{e}")
    chosen_resp = rejected_resp.replace(ORG_NAME, POS_NAME)#生成选择响应
    chosen = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": chosen_resp},
    ]
    rejected = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": rejected_resp},
    ]

    return {"chosen": chosen, "rejected": rejected}

通过generate_responses来让模型根据mrfakename/identity数据集中的这些关于身份的问题来生成回答作为负例写入到rejected，然后用replace把模型的回复中所有想替换的字段进行更换

这样一来就完成了一个将qwen模型的身份修改为Deep Qwen

同样通过DPOConfig来设置训练参数
config = DPOConfig(
    beta=0.2, # beta参数控制选择和拒绝响应的权重
    per_device_train_batch_size=1,# 每个设备的训练批次大小
    gradient_accumulation_steps=8,# 梯度累积步数
    num_train_epochs=1,# 训练的总轮数
    learning_rate=5e-5,# 学习率
    logging_steps=2,# 日志记录步数
)
其中beta的大小非常重要，一般就是0.1-0.2不然就会发生过拟合，梯度爆炸，丢失 SFT 能力
小 β = 稳定对齐，不破坏模型能力。
大 β = 强制对齐，风险是失去创造力和知识。

dpo_trainer = DPOTrainer(
    model=model,# 模型
    ref_model=None,# 参考模型（如果有的话）
    args=config,    # 训练参数配置
    processing_class=tokenizer,  # 分词器
    train_dataset=dpo_ds# 训练数据集
)
#训练DPO模型
dpo_trainer.train()
即可开始训练，这里ref_model默认为原模型的权重矩阵，不会更新，是用来保证模型在不偏离原模型的情况下进行更新
